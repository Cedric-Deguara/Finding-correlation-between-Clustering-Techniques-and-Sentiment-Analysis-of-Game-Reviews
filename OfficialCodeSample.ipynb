{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn import svm\n",
    "from sklearn.utils import resample\n",
    "from scipy.sparse import vstack\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.cluster import KMeans\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import numpy as np\n",
    "from sklearn import svm\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.model_selection import ParameterSampler\n",
    "from sklearn.utils import resample\n",
    "from tqdm import tqdm\n",
    "from joblib import Parallel, delayed\n",
    "from scipy.sparse import vstack\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from kneed import KneeLocator\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.cluster import DBSCAN\n",
    "from itertools import product\n",
    "\n",
    "from collections import Counter\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "from sklearn.manifold import TSNE\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "from wordcloud import WordCloud\n",
    "import matplotlib.patches as mpatches\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adjust the path to where your dataset is stored\n",
    "dataset = pd.read_csv('dataset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distinct_games = dataset['app_name'].drop_duplicates()\n",
    "selected_games = distinct_games.sample(10, random_state=42) \n",
    "\n",
    "sample_rows = pd.DataFrame()\n",
    "\n",
    "for game in selected_games:\n",
    "    sample_rows = sample_rows.append(dataset[dataset['app_name'] == game].sample(1, random_state=42))\n",
    "\n",
    "print(sample_rows)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set display options\n",
    "pd.set_option('display.expand_frame_repr', False)\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "# Randomly select 10 different games\n",
    "distinct_games = dataset['app_name'].drop_duplicates()\n",
    "selected_games = distinct_games.sample(10, random_state=42) \n",
    "\n",
    "# Get one review from each of the selected games\n",
    "sample_rows = pd.DataFrame()\n",
    "\n",
    "for game in selected_games:\n",
    "    sample_rows = sample_rows.append(dataset[dataset['app_name'] == game].sample(1, random_state=42))\n",
    "\n",
    "# Print the selected rows\n",
    "print(sample_rows)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Histogram for 'review_score' column\n",
    "plt.figure(figsize=(10,6))\n",
    "sns.histplot(data=dataset, x='review_score', kde=True)\n",
    "plt.title('Distribution of Review Scores')\n",
    "plt.show()\n",
    "\n",
    "# Bar chart for 'review_votes' column\n",
    "plt.figure(figsize=(10,6))\n",
    "sns.countplot(x='review_votes', data=dataset)\n",
    "plt.title('Distribution of Review Votes')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get frequency counts of 'review_score' column\n",
    "review_score_counts = dataset['review_score'].value_counts()\n",
    "print(\"Frequency counts of review scores:\")\n",
    "print(review_score_counts)\n",
    "\n",
    "# Get frequency counts of 'review_votes' column\n",
    "review_votes_counts = dataset['review_votes'].value_counts()\n",
    "print(\"\\nFrequency counts of review votes:\")\n",
    "print(review_votes_counts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dataset['app_name'].value_counts())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop the rows with missing 'app_name' or 'review_text'\n",
    "dataset.dropna(subset=['app_name', 'review_text'], inplace=True)\n",
    "\n",
    "# First, create a separate dataframe with games having more 1s than 0s in review_votes.\n",
    "games_with_more_1s = dataset[dataset['review_votes'] == 1].groupby('app_id').filter(lambda x: x['review_votes'].sum() > len(x) / 2)\n",
    "\n",
    "# Now create a balanced dataset with equal number of positive and negative reviews\n",
    "positive_reviews = games_with_more_1s[games_with_more_1s['review_score'] == 1]\n",
    "negative_reviews = games_with_more_1s[games_with_more_1s['review_score'] == -1]\n",
    "\n",
    "# Find the minimum size between positive and negative reviews\n",
    "min_size = min(len(positive_reviews), len(negative_reviews))\n",
    "\n",
    "# Sample from the larger dataset to match the size of the smaller dataset\n",
    "positive_reviews_balanced = positive_reviews.sample(min_size)\n",
    "negative_reviews_balanced = negative_reviews.sample(min_size)\n",
    "\n",
    "# Concatenate the positive and negative reviews to form a balanced dataset\n",
    "balanced_reviews = pd.concat([positive_reviews_balanced, negative_reviews_balanced])\n",
    "\n",
    "# Shuffle the dataset\n",
    "balanced_reviews = balanced_reviews.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "# Show the distribution of review_scores in the balanced dataset\n",
    "print(balanced_reviews['review_score'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the number of samples\n",
    "n_samples = 50000 \n",
    "\n",
    "# Sample from the balanced positive and negative reviews\n",
    "positive_reviews_sampled = positive_reviews_balanced.sample(n_samples)\n",
    "negative_reviews_sampled = negative_reviews_balanced.sample(n_samples)\n",
    "\n",
    "# Concatenate the positive and negative reviews to form a balanced and reduced dataset\n",
    "balanced_reviews_reduced = pd.concat([positive_reviews_sampled, negative_reviews_sampled])\n",
    "\n",
    "# Shuffle the dataset\n",
    "balanced_reviews_reduced = balanced_reviews_reduced.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "# Show the distribution of review_scores in the reduced dataset\n",
    "print(balanced_reviews_reduced['review_score'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Review the structure of the balanced dataset\n",
    "print(\"\\nStructure of the dataset:\")\n",
    "print(balanced_reviews_reduced.info())\n",
    "\n",
    "# Check for missing values\n",
    "print(\"\\nMissing values in each column:\")\n",
    "print(balanced_reviews_reduced.isnull().sum())\n",
    "\n",
    "# Show the number of unique games in the balanced dataset\n",
    "print(\"\\nNumber of unique games: \", balanced_reviews_reduced['app_id'].nunique())\n",
    "\n",
    "# Show the number of unique reviews in the balanced dataset\n",
    "print(\"\\nNumber of unique reviews: \", balanced_reviews_reduced['review_text'].nunique())\n",
    "\n",
    "# Show the distribution of review_scores\n",
    "print(\"\\nDistribution of review scores: \")\n",
    "print(balanced_reviews_reduced['review_score'].value_counts())\n",
    "\n",
    "# Show the distribution of review_votes\n",
    "print(\"\\nDistribution of review votes: \")\n",
    "print(balanced_reviews_reduced['review_votes'].value_counts())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove rows where 'review_text' contains 'early access reviews'\n",
    "dataset= dataset[~dataset['review_text'].str.contains('early access reviews')]\n",
    "# Remove rows where 'review_text' contains 'early access reviews'\n",
    "dataset = dataset[~dataset['review_text'].str.contains('early access review')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the cleaned dataset to a new CSV file\n",
    "balanced_reviews_reduced.to_csv('cleaned_dataset.csv', index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read CSV file\n",
    "df = pd.read_csv('cleaned_dataset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    text = re.sub('[^a-zA-Z]', ' ', text)  # Keep only alphabets\n",
    "    text = text.lower()  # Convert to lowercase\n",
    "    return text\n",
    "\n",
    "# Perform text cleaning\n",
    "df['review_text'] = df['review_text'].apply(clean_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['review_text'] = df['review_text'].apply(nltk.word_tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def remove_stopwords(text):\n",
    "    return [word for word in text if word not in stop_words]\n",
    "\n",
    "df['review_text'] = df['review_text'].apply(remove_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def word_lemmatizer(text):\n",
    "    return [lemmatizer.lemmatize(word) for word in text]\n",
    "\n",
    "df['review_text'] = df['review_text'].apply(word_lemmatizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def join_text(text):\n",
    "    return ' '.join(text)\n",
    "\n",
    "df['review_text'] = df['review_text'].apply(join_text)\n",
    "\n",
    "vectorizer = TfidfVectorizer()\n",
    "X = vectorizer.fit_transform(df['review_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the cleaned dataset to a new CSV file\n",
    "df.to_csv('preprocessedCleanedData.csv', index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentiment Analysis Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data\n",
    "df = pd.read_csv('preprocessedCleanedData.csv')\n",
    "\n",
    "# Split the data into training and testing sets using stratified sampling\n",
    "X_temp, X_test, y_temp, y_test = train_test_split(X, y, test_size=0.20, random_state=42, stratify=y)\n",
    "\n",
    "# Split the remaining data into training and validation sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_temp, y_temp, test_size=0.15, random_state=42, stratify=y_temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a Naive Bayes model\n",
    "mnb = MultinomialNB()\n",
    "\n",
    "# Define a hyperparameter grid to search over\n",
    "param_grid = {\n",
    "    'alpha': [0.1, 0.5, 1, 5, 10],\n",
    "    'fit_prior': [True, False],\n",
    "    # Assuming you have 2 classes, change [0.5, 0.5] accordingly if you have a different number of classes\n",
    "    'class_prior': [[0.1, 0.9], [0.2, 0.8], [0.3, 0.7], [0.4, 0.6], [0.5, 0.5], [0.6, 0.4], [0.7, 0.3], [0.8, 0.2], [0.9, 0.1], None]\n",
    "}\n",
    "\n",
    "# Set up the grid search\n",
    "grid_search = GridSearchCV(mnb, param_grid, cv=5)\n",
    "\n",
    "# Fit the grid search to the training data\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Get the best parameters\n",
    "best_params = grid_search.best_params_\n",
    "print(best_params)\n",
    "\n",
    "# Train a new MultinomialNB with the best parameters\n",
    "mnb_best = MultinomialNB(**best_params)\n",
    "mnb_best.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validate the model using validation set\n",
    "y_val_pred = mnb_best.predict(X_val)\n",
    "val_accuracy = accuracy_score(y_val, y_val_pred)\n",
    "print(f'Validation Accuracy: {val_accuracy}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Additional metrics like precision, recall and f1-score\n",
    "print('\\nValidation Classification Report:')\n",
    "print(classification_report(y_val, y_val_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a SVM model\n",
    "svc = svm.SVC()\n",
    "\n",
    "# Define a hyperparameter grid to search over\n",
    "svm_params = {\n",
    "    \"kernel\": [\"linear\", \"poly\", \"rbf\", \"sigmoid\"],\n",
    "    \"C\": [0.1, 1, 10],\n",
    "    \"gamma\": ['scale', 'auto'] + list(np.logspace(-4, 2, 10)),\n",
    "    \"degree\": [2, 3, 4],\n",
    "    \"coef0\": [0, 1, 2],\n",
    "    \"shrinking\": [True, False],\n",
    "    \"probability\": [True],\n",
    "    \"tol\": [1e-4, 1e-3, 1e-2],\n",
    "    \"class_weight\": [None, \"balanced\"]\n",
    "}\n",
    "\n",
    "# Get a balanced sample of the training data\n",
    "sample_size = int(X_train.shape[0] * 0.5)\n",
    "X_train_sample_neg = resample(X_train[y_train == -1], n_samples=sample_size // 2, replace=False)\n",
    "X_train_sample_pos = resample(X_train[y_train == 1], n_samples=sample_size // 2, replace=False)\n",
    "X_train_sample = vstack([X_train_sample_neg, X_train_sample_pos])\n",
    "y_train_sample = np.concatenate([-np.ones(sample_size // 2), np.ones(sample_size // 2)])\n",
    "\n",
    "# Implement the progress bar function\n",
    "def progress_bar_cross_val_score(estimator, X, y, cv):\n",
    "    scores = []\n",
    "    for train_index, test_index in tqdm(cv.split(X, y), desc=\"CV\", leave=False):\n",
    "        X_train_fold, X_test_fold = X[train_index], X[test_index]\n",
    "        y_train_fold, y_test_fold = y[train_index], y[test_index]\n",
    "        estimator.fit(X_train_fold, y_train_fold)\n",
    "        y_pred = estimator.predict(X_test_fold)\n",
    "        score = f1_score(y_test_fold, y_pred)\n",
    "        scores.append(score)\n",
    "    return scores\n",
    "\n",
    "# Define a function to evaluate a set of parameters\n",
    "def evaluate_params(params):\n",
    "    svc.set_params(**params)\n",
    "    scores = progress_bar_cross_val_score(svc, X_train_sample, y_train_sample, cv=kf)\n",
    "    mean_score = np.mean(scores)\n",
    "    return mean_score, params\n",
    "\n",
    "# Perform the parameter search using KFold and progress bar\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "param_list = list(ParameterSampler(svm_params, n_iter=30, random_state=42))\n",
    "\n",
    "best_score_params = Parallel(n_jobs=-1)(\n",
    "    delayed(evaluate_params)(params) for params in tqdm(param_list, desc=\"Parameters\"))\n",
    "\n",
    "best_score, best_params = max(best_score_params, key=lambda x: x[0])\n",
    "\n",
    "# Print the best parameters\n",
    "print(f\"Best parameters: {best_params}\")\n",
    "\n",
    "# Train the SVM with the best parameters on the entire training set\n",
    "best_svm = svm.SVC(**best_params)\n",
    "best_svm.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validate the model using validation set\n",
    "y_val_pred = best_svm.predict(X_val)\n",
    "val_accuracy = accuracy_score(y_val, y_val_pred)\n",
    "print(f'Validation Accuracy: {val_accuracy}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Additional metrics like precision, recall and f1-score\n",
    "print('\\nValidation Classification Report:')\n",
    "print(classification_report(y_val, y_val_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the model using testing set\n",
    "y_test_pred = mnb_best.predict(X_test)\n",
    "test_accuracy = accuracy_score(y_test, y_test_pred)\n",
    "print(f'Test Accuracy: {test_accuracy}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\nTest Classification Report:')\n",
    "print(classification_report(y_test, y_test_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the model using test set\n",
    "y_test_pred = best_svm.predict(X_test)\n",
    "test_accuracy = accuracy_score(y_test, y_test_pred)\n",
    "print(f'Test Accuracy: {test_accuracy}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a classification report\n",
    "print('Test Classification Report:')\n",
    "print(classification_report(y_test, y_test_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the SVM model using test set\n",
    "y_test_pred_svm = best_svm.predict(X_test)\n",
    "test_accuracy_svm = accuracy_score(y_test, y_test_pred_svm)\n",
    "print(f'SVM Test Accuracy: {test_accuracy_svm}')\n",
    "\n",
    "# Generate confusion matrix for SVM\n",
    "conf_mat_svm = confusion_matrix(y_test, y_test_pred_svm)\n",
    "\n",
    "# Plot confusion matrix for SVM as a heatmap\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(conf_mat_svm, annot=True, fmt='d', cmap='Blues', xticklabels=['Negative', 'Positive'], yticklabels=['Negative', 'Positive'])\n",
    "plt.title('SVM Confusion Matrix')\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.ylabel('True Label')\n",
    "plt.show()\n",
    "\n",
    "# Test the MNB model using test set\n",
    "y_test_pred_mnb = mnb_best.predict(X_test)\n",
    "test_accuracy_mnb = accuracy_score(y_test, y_test_pred_mnb)\n",
    "print(f'MNB Test Accuracy: {test_accuracy_mnb}')\n",
    "\n",
    "# Generate confusion matrix for MNB\n",
    "conf_mat_mnb = confusion_matrix(y_test, y_test_pred_mnb)\n",
    "\n",
    "# Plot confusion matrix for MNB as a heatmap\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(conf_mat_mnb, annot=True, fmt='d', cmap='Blues', xticklabels=['Negative', 'Positive'], yticklabels=['Negative', 'Positive'])\n",
    "plt.title('Naive Bayes Confusion Matrix')\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.ylabel('True Label')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate prediction probabilities\n",
    "y_test_proba_svm = best_svm.predict_proba(X_test)[:, 1]\n",
    "y_test_proba_mnb = mnb_best.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Compute ROC curve for SVM\n",
    "fpr_svm, tpr_svm, _ = roc_curve(y_test, y_test_proba_svm)\n",
    "roc_auc_svm = auc(fpr_svm, tpr_svm)\n",
    "\n",
    "# Compute ROC curve for MNB\n",
    "fpr_mnb, tpr_mnb, _ = roc_curve(y_test, y_test_proba_mnb)\n",
    "roc_auc_mnb = auc(fpr_mnb, tpr_mnb)\n",
    "\n",
    "# Plot ROC curves\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(fpr_svm, tpr_svm, label='SVM (area = %0.2f)' % roc_auc_svm)\n",
    "plt.plot([0, 1], [0, 1], color='navy', linestyle='--')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver Operating Characteristic for SVM')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()\n",
    "\n",
    "# Plot ROC curves\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(fpr_mnb, tpr_mnb, label='MNB (area = %0.2f)' % roc_auc_mnb)\n",
    "plt.plot([0, 1], [0, 1], color='navy', linestyle='--')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver Operating Characteristic for Naive Bayes')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clustering Algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use trained models to predict sentiment labels on test set\n",
    "y_val_pred_mnb = mnb_best.predict(X_val)\n",
    "y_val_pred_svm = best_svm.predict(X_val)\n",
    "\n",
    "# Add these labels to your test dataframe\n",
    "df_test = df.loc[df.index.isin(test_indices)].copy()\n",
    "df_test['mnb_sentiment'] = y_val_pred_mnb\n",
    "df_test['svm_sentiment'] = y_val_pred_svm\n",
    "\n",
    "# Print the number of instances classified as negative and positive by each model\n",
    "print(\"SVM: Number of negative instances =\", sum(df_test['svm_sentiment'] == -1))\n",
    "print(\"SVM: Number of positive instances =\", sum(df_test['svm_sentiment'] == 1))\n",
    "print(\"MNB: Number of negative instances =\", sum(df_test['mnb_sentiment'] == -1))\n",
    "print(\"MNB: Number of positive instances =\", sum(df_test['mnb_sentiment'] == 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add sentiment labels to the test set\n",
    "df_val = df.loc[test_indices].copy()\n",
    "df_val['mnb_sentiment'] = y_val_pred_mnb\n",
    "df_val['svm_sentiment'] = y_val_pred_svm\n",
    "\n",
    "# Split test data into 4 datasets: SVM Positive, SVM Negative, MNB Positive, MNB Negative\n",
    "datasets = [df_val[df_val['svm_sentiment'] == label] for label in [-1, 1]] + \\\n",
    "           [df_val[df_val['mnb_sentiment'] == label] for label in [-1, 1]]\n",
    "dataset_names = ['SVM Negative', 'SVM Positive', 'Naive Bayes Negative', 'Naive Bayes Positive']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare to record the best number of clusters for each dataset\n",
    "best_ks = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature extraction\n",
    "df_val['review_length'] = df_val['review'].apply(lambda x: len(x))\n",
    "df_val['word_count'] = df_val['review'].apply(lambda x: len(x.split()))\n",
    "\n",
    "# Feature normalization\n",
    "scaler = MinMaxScaler()\n",
    "df_val[['review_length', 'word_count']] = scaler.fit_transform(df_val[['review_length', 'word_count']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop over datasets\n",
    "for df_temp, name in zip(datasets, dataset_names):\n",
    "    # Check if the dataframe is empty\n",
    "    if df_temp.empty:\n",
    "        print(f\"No instances of {name} in the test set.\")\n",
    "        best_ks.append(None)\n",
    "        continue    \n",
    "    # Get the group's data\n",
    "    group_data = df_val.loc[df_temp.index, ['review_length', 'word_count']]\n",
    "\n",
    "    # Use the elbow method to find the best number of clusters\n",
    "    distortions = []\n",
    "    K = range(1,12)\n",
    "    for k in K:\n",
    "        kmeanModel = KMeans(n_clusters=k)\n",
    "        kmeanModel.fit(group_data)\n",
    "        distortions.append(kmeanModel.inertia_)\n",
    "\n",
    "    # Plot the elbow\n",
    "    plt.figure(figsize=(15,8))\n",
    "    plt.plot(K, distortions, 'bx-')\n",
    "    plt.xlabel('k')\n",
    "    plt.ylabel('Distortion')\n",
    "    plt.title('The Elbow Method showing the optimal k for ' + name)\n",
    "    plt.show()\n",
    "\n",
    "    # Assuming the \"elbow\" is located at the point where the plot bends most sharply,\n",
    "    # we can determine this by finding the point with the maximum curvature.\n",
    "    kneedle = KneeLocator(K, distortions, curve='convex', direction='decreasing')\n",
    "    print(f'The optimal number of clusters for {name} is {kneedle.knee}')\n",
    "    best_ks.append(kneedle.knee)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a mapping of original indices to X_test indices\n",
    "idx_mapping = {idx: i for i, idx in enumerate(test_indices)}\n",
    "\n",
    "# Now we perform k-means clustering for each group using 3 clusters\n",
    "clusterings = {}\n",
    "dataset_names = ['SVM Negative', 'SVM Positive', 'Naive Bayes Negative', 'Naive Bayes Positive']\n",
    "\n",
    "# Initialize TruncatedSVD\n",
    "svd = TruncatedSVD(n_components=100, random_state=42)\n",
    "\n",
    "for name in dataset_names:\n",
    "    # Only proceed if there is data for this group\n",
    "    if not datasets[dataset_names.index(name)].empty:\n",
    "        # Get the indices of the reviews in this group\n",
    "        group_indices = datasets[dataset_names.index(name)].index\n",
    "        # Map original indices to X_test indices\n",
    "        mapped_indices = [idx_mapping[idx] for idx in group_indices]\n",
    "        # Get the corresponding TF-IDF vectors\n",
    "        group_data = X_val[mapped_indices]\n",
    "        # Perform dimensionality reduction\n",
    "        group_data_reduced = svd.fit_transform(group_data)\n",
    "        # Perform k-means clustering\n",
    "        kmeans = KMeans(n_clusters=3, random_state=42)\n",
    "        kmeans.fit(group_data_reduced)\n",
    "        clusterings[name] = kmeans.labels_\n",
    "        silhouette_avg = silhouette_score(group_data_reduced, kmeans.labels_)\n",
    "        print(f\"For {name}, the average silhouette score is : {silhouette_avg}\")\n",
    "\n",
    "# Assign the clusters to the original test set\n",
    "for name, clustering in clusterings.items():\n",
    "    df_val.loc[datasets[dataset_names.index(name)].index, name + '_cluster'] = clustering\n",
    "\n",
    "print(df_val.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the parameter space\n",
    "eps_values = [0.1, 0.5, 1.0, 1.5, 2.0]  \n",
    "min_samples_values = [5, 10, 15, 20]  \n",
    "\n",
    "best_params = {}\n",
    "for name in dataset_names:\n",
    "    if not datasets[dataset_names.index(name)].empty:\n",
    "        group_indices = datasets[dataset_names.index(name)].index\n",
    "        mapped_indices = [idx_mapping[idx] for idx in group_indices]\n",
    "        group_data = X_test[mapped_indices]\n",
    "        group_data_reduced = svd.fit_transform(group_data)\n",
    "\n",
    "        # Initialize the best score for this group\n",
    "        best_score = -1\n",
    "        # Iterate over all combinations of eps_values and min_samples_values\n",
    "        for eps, min_samples in product(eps_values, min_samples_values):\n",
    "            dbscan = DBSCAN(eps=eps, min_samples=min_samples)\n",
    "            labels = dbscan.fit_predict(group_data_reduced)\n",
    "            \n",
    "            # Ignore the -1 labels as they are considered noise by DBSCAN\n",
    "            core_samples_mask = labels != -1\n",
    "            labels_core = labels[core_samples_mask]\n",
    "            group_data_core = group_data_reduced[core_samples_mask]\n",
    "            \n",
    "            # Compute the silhouette score\n",
    "            if len(set(labels_core)) > 1:  # to avoid ValueError\n",
    "                score = silhouette_score(group_data_core, labels_core)\n",
    "                if score > best_score:\n",
    "                    best_score = score\n",
    "                    best_params[name] = {'eps': eps, 'min_samples': min_samples}\n",
    "\n",
    "# Print the best parameters\n",
    "for name, params in best_params.items():\n",
    "    print(name, params)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the parameter space\n",
    "eps_values = [0.1, 0.5, 1.0, 1.5, 2.0]  \n",
    "min_samples_values = [5, 10, 15, 20]  \n",
    "\n",
    "best_params = {}\n",
    "for name in dataset_names:\n",
    "    if not datasets[dataset_names.index(name)].empty:\n",
    "        group_indices = datasets[dataset_names.index(name)].index\n",
    "        mapped_indices = [idx_mapping[idx] for idx in group_indices]\n",
    "        group_data = X_test[mapped_indices]\n",
    "        group_data_reduced = svd.fit_transform(group_data)\n",
    "\n",
    "        # Initialize the best score for this group\n",
    "        best_score = -1\n",
    "        # Iterate over all combinations of eps_values and min_samples_values\n",
    "        for eps, min_samples in product(eps_values, min_samples_values):\n",
    "            dbscan = DBSCAN(eps=eps, min_samples=min_samples)\n",
    "            labels = dbscan.fit_predict(group_data_reduced)\n",
    "            \n",
    "            # Ignore the -1 labels as they are considered noise by DBSCAN\n",
    "            core_samples_mask = labels != -1\n",
    "            labels_core = labels[core_samples_mask]\n",
    "            group_data_core = group_data_reduced[core_samples_mask]\n",
    "            \n",
    "            # Compute the silhouette score\n",
    "            if len(set(labels_core)) > 1:  # to avoid ValueError\n",
    "                score = silhouette_score(group_data_core, labels_core)\n",
    "                if score > best_score:\n",
    "                    best_score = score\n",
    "                    best_params[name] = {'eps': eps, 'min_samples': min_samples}\n",
    "\n",
    "# Print the best parameters\n",
    "for name, params in best_params.items():\n",
    "    print(name, params)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize CountVectorizer\n",
    "vec = CountVectorizer(stop_words='english', max_features=10)\n",
    "\n",
    "# Loop over each group and each cluster within the group\n",
    "for name in dataset_names:\n",
    "    for cluster in range(3):\n",
    "        # Get the reviews in this cluster\n",
    "        reviews = df_val[(df_val[name + '_cluster'] == cluster)]['review_text']\n",
    "        if reviews.empty:\n",
    "            continue\n",
    "        # Get the most common words in these reviews\n",
    "        word_counts = Counter()\n",
    "        for _, review in reviews.iteritems():\n",
    "            words = review.split()\n",
    "            word_counts.update(words)\n",
    "        # Print the 10 most common words\n",
    "        print(f'The 10 most common words in cluster {cluster} of {name} are:')\n",
    "        for word, count in word_counts.most_common(10):\n",
    "            print(f'{word}: {count}')\n",
    "        print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For each sentiment model and each cluster, calculate the average sentiment score\n",
    "for name in dataset_names:\n",
    "    for cluster in range(3):\n",
    "        avg_score = df_val[df_val[name + '_cluster'] == cluster]['review_score'].mean()\n",
    "        print(f\"Average sentiment score for cluster {cluster} in {name}: {avg_score:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the best number of clusters for each sentiment of each model\n",
    "best_ks = [3, 8, 7, 7]\n",
    "\n",
    "# Your loop where you analyze each dataset\n",
    "for i, (dataset, name) in enumerate(zip(datasets, dataset_names)):\n",
    "    print(f'\\n{name}:')\n",
    "\n",
    "    # Create a copy of the dataset to avoid SettingWithCopyWarning\n",
    "    dataset = dataset.copy()\n",
    "\n",
    "    # Transform the text to vectors\n",
    "    X = vectorizer.transform(dataset['review_text'])\n",
    "    X_svd = svd.fit_transform(X)\n",
    "\n",
    "    num_clusters_kmeans = best_ks[i]\n",
    "    num_clusters_dbscan = len(np.unique(dbscan.labels_))\n",
    "    \n",
    "    if num_clusters_kmeans:\n",
    "        # Perform KMeans clustering with n_init set explicitly to avoid FutureWarning\n",
    "        kmeans = KMeans(n_clusters=num_clusters_kmeans, n_init=10, random_state=42).fit(X_svd)\n",
    "        dataset['kmeans_cluster'] = kmeans.labels_\n",
    "        print(f\"Number of clusters created by KMeans: {len(np.unique(kmeans.labels_))}\")\n",
    "\n",
    "    # Perform DBSCAN clustering\n",
    "    eps = distances[-1] / 2\n",
    "    min_samples = 2 * X_svd.shape[1]\n",
    "    dbscan = DBSCAN(eps=eps, min_samples=min_samples).fit(X_svd)\n",
    "    dataset['dbscan_cluster'] = dbscan.labels_\n",
    "    print(f\"Number of clusters created by DBSCAN: {num_clusters_dbscan}\")\n",
    "\n",
    "    # Calculate average sentiment for each cluster\n",
    "    if num_clusters_kmeans:\n",
    "        kmeans_avg_sentiment_svm = dataset.groupby('kmeans_cluster')['svm_sentiment'].mean()\n",
    "        kmeans_avg_sentiment_mnb = dataset.groupby('kmeans_cluster')['mnb_sentiment'].mean()\n",
    "        print(\"Average SVM sentiment for each KMeans cluster:\")\n",
    "        print(kmeans_avg_sentiment_svm)\n",
    "        print(\"Average MNB sentiment for each KMeans cluster:\")\n",
    "        print(kmeans_avg_sentiment_mnb)\n",
    "\n",
    "    dbscan_avg_sentiment_svm = dataset.groupby('dbscan_cluster')['svm_sentiment'].mean()\n",
    "    dbscan_avg_sentiment_mnb = dataset.groupby('dbscan_cluster')['mnb_sentiment'].mean()\n",
    "    print(\"Average SVM sentiment for each DBSCAN cluster:\")\n",
    "    print(dbscan_avg_sentiment_svm)\n",
    "    print(\"Average MNB sentiment for each DBSCAN cluster:\")\n",
    "    print(dbscan_avg_sentiment_mnb)\n",
    "\n",
    "    # PCA and plot\n",
    "    pca = PCA(n_components=2)\n",
    "    principalComponents = pca.fit_transform(X_svd)\n",
    "\n",
    "    if num_clusters_kmeans:\n",
    "        cmap = plt.get_cmap('gist_rainbow')\n",
    "        legend_patches = [mpatches.Patch(color=cmap(i/num_clusters_kmeans), label=f'Cluster {i}') for i in range(num_clusters_kmeans)]\n",
    "        plt.legend(handles=legend_patches)\n",
    "        plt.scatter(principalComponents[:, 0], principalComponents[:, 1], c=dataset['kmeans_cluster'], cmap=cmap)\n",
    "        plt.title('KMeans Clusters')\n",
    "        plt.show()\n",
    "\n",
    "    cmap_dbscan = plt.get_cmap('viridis')\n",
    "    legend_patches_dbscan = [mpatches.Patch(color=cmap_dbscan(i/num_clusters_dbscan), label=(f'Cluster {i}' if i >= 0 else 'Noise')) for i in np.unique(dbscan.labels_)]\n",
    "    plt.legend(handles=legend_patches_dbscan)\n",
    "    plt.scatter(principalComponents[:, 0], principalComponents[:, 1], c=dataset['dbscan_cluster'], cmap=cmap_dbscan)\n",
    "    plt.title('DBSCAN Clusters')\n",
    "    plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
